defaults:
  - finetune_base

name: "${base_name} - finetune head"
multivariate_tpe: false
key_metric: "Acc"
tuning_params:
  floats:
    head_lr:
      log: True
      low: 1.0e-6
      high: 1.0e-2
  ints: {}
  categoricals:
    #dirichlet_config.blend_with_uniform: [false, true]
    dirichlet_config.alpha: [0.2, 0.7]
    dirichlet_config.tau: [1.0, 0.8, 0.6]

train_config:
  dataloader_num_workers: 0
  prefetch_factor: 2
  optimizer: "AdamW"
  head_lr: 1e-5
  backbone_lr: 0
  batch_size: 128
  epochs: 200
  patience: 15
  stop_early: True
  loss_config: 
    name: "CE"          # cross-entropy
  # --- Dirichlet prior augmentation ---
  dirichlet_config:
    enabled: true
    alpha: 0.7
    tau: 1.0
    blend_with_uniform: false
    beta: 0.8  
